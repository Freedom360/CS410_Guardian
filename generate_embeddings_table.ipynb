{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required imports\n",
    "import os\n",
    "import langchain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import bs4\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "import pandas as pd\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import numpy as np\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Week</th>\n",
       "      <th>Lesson</th>\n",
       "      <th>Chunk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>introduction video from previous semester</td>\n",
       "      <td>Hello welcome to CS410 DSO Text Information Sy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>introduction video from previous semester</td>\n",
       "      <td>news articles, or Emails and other kind of doc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>introduction video from previous semester</td>\n",
       "      <td>the Text Retrieval and Text Mining. And these ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>introduction video from previous semester</td>\n",
       "      <td>two steps corresponding to Text Retrieval and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>introduction video from previous semester</td>\n",
       "      <td>because those books have covered a general tec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236</th>\n",
       "      <td>12</td>\n",
       "      <td>8 summary for exam 2</td>\n",
       "      <td>why statistical learning is important. We also...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>12</td>\n",
       "      <td>8 summary for exam 2</td>\n",
       "      <td>are interested in building practical text appl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>12</td>\n",
       "      <td>8 summary for exam 2</td>\n",
       "      <td>the original text to verify that. And that is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>12</td>\n",
       "      <td>8 summary for exam 2</td>\n",
       "      <td>text retrieval and text mining. And text retri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1240</th>\n",
       "      <td>12</td>\n",
       "      <td>8 summary for exam 2</td>\n",
       "      <td>I just would like to thank you for taking this...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1241 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Week                                     Lesson  \\\n",
       "0        1  introduction video from previous semester   \n",
       "1        1  introduction video from previous semester   \n",
       "2        1  introduction video from previous semester   \n",
       "3        1  introduction video from previous semester   \n",
       "4        1  introduction video from previous semester   \n",
       "...    ...                                        ...   \n",
       "1236    12                       8 summary for exam 2   \n",
       "1237    12                       8 summary for exam 2   \n",
       "1238    12                       8 summary for exam 2   \n",
       "1239    12                       8 summary for exam 2   \n",
       "1240    12                       8 summary for exam 2   \n",
       "\n",
       "                                                  Chunk  \n",
       "0     Hello welcome to CS410 DSO Text Information Sy...  \n",
       "1     news articles, or Emails and other kind of doc...  \n",
       "2     the Text Retrieval and Text Mining. And these ...  \n",
       "3     two steps corresponding to Text Retrieval and ...  \n",
       "4     because those books have covered a general tec...  \n",
       "...                                                 ...  \n",
       "1236  why statistical learning is important. We also...  \n",
       "1237  are interested in building practical text appl...  \n",
       "1238  the original text to verify that. And that is ...  \n",
       "1239  text retrieval and text mining. And text retri...  \n",
       "1240  I just would like to thank you for taking this...  \n",
       "\n",
       "[1241 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#process text files\n",
    "data = []\n",
    "\n",
    "#go through each lecture and get metadata\n",
    "for i in range(1, 13):\n",
    "    directory = f\"{i:02d}_week-{i}\"\n",
    "    \n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".txt\"):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            \n",
    "            # Extract the week number and lesson name from the filename\n",
    "            parts = file.split(\"_\", 3)\n",
    "            week_number = int(parts[0])\n",
    "            lesson_name = parts[-1].replace(\".en.txt\", \"\").replace(\"-\", \" \").split(\" \", 1)[-1]\n",
    "            \n",
    "            try:\n",
    "                # Load the text files\n",
    "                loader = TextLoader(file_path, encoding=\"utf-8\")\n",
    "                docs = loader.load()\n",
    "                \n",
    "                # Split the document into chunks\n",
    "                text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "                splits = text_splitter.split_documents(docs)\n",
    "                \n",
    "                # Add each chunk and metadata to array\n",
    "                for split in splits:\n",
    "                    data.append({\n",
    "                        \"Week\": week_number,\n",
    "                        \"Lesson\": lesson_name,\n",
    "                        \"Chunk\": split.page_content\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file_path}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "# final dataframe containing week, lesson, and chunk columns\n",
    "content = pd.DataFrame(data)\n",
    "\n",
    "content.to_csv(\"content.csv\", index=False)\n",
    "\n",
    "# lessons dataframe with unique lessons for each week\n",
    "lessons = content[[\"Week\", \"Lesson\"]].drop_duplicates()\n",
    "\n",
    "# save to csv\n",
    "lessons.to_csv(\"lessons.csv\", index=False)\n",
    "\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zb5pf6v\\AppData\\Local\\Temp\\ipykernel_3244\\763944131.py:14: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  ali = HuggingFaceEmbeddings(\n",
      "C:\\Users\\zb5pf6v\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#set up huggingface embeddings\n",
    "device = \"cpu\"\n",
    "\n",
    "model_name = \"Alibaba-NLP/gte-large-en-v1.5\"\n",
    "model_kwargs = {\"device\": device, \"trust_remote_code\": True}\n",
    "encode_kwargs = {\n",
    "    \"normalize_embeddings\": False,\n",
    "    \"batch_size\": 512\n",
    "}\n",
    "# initialize\n",
    "ali = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate embeddings for Chunk column and save to csv\n",
    "content[\"vectors\"] = ali.embed_documents(content[\"Chunk\"])\n",
    "content.to_csv(\"content_vectors.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate embeddings for Lesson column and save to csv\n",
    "lessons[\"vectors\"] = ali.embed_documents(lessons[\"Lesson\"])\n",
    "lessons.to_csv(\"lessons_vectors.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create embeddings table used for vector store\n",
    "# pc_col = custom user-defined column name for column that gets converted to embeddings within 'df'\n",
    "\n",
    "def vector_store_faiss(df, embeddings, embeddings_model, pc_col = False, metadata_cols = False):\n",
    "    # converting metadata to dictionary for FAISS\n",
    "    metadata = df.to_dict(orient='records') if metadata_cols == False else df[metadata_cols].to_dict(orient='records')\n",
    "\n",
    "    #extract text data\n",
    "    # if pc_col is false use Concatenated column, otherwise pc_col\n",
    "    texts = df['Concatenated'].tolist() if pc_col == False else df[pc_col].tolist()\n",
    "    \n",
    "    # pair text data with embeddings\n",
    "    text_embedding_pairs = zip(texts, embeddings)\n",
    "\n",
    "    #create table\n",
    "    vector_store = FAISS.from_embeddings(text_embedding_pairs, embeddings_model, metadatas = metadata)\n",
    "    \n",
    "    return vector_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_content = [arr.tolist() for arr in content['vectors']]\n",
    "\n",
    "# create embeddings table for content from Chunk column, drop redundant data in vectors column and convert to list for correct format\n",
    "vs_content = vector_store_faiss(content.drop(columns = ['vectors']), content['vectors'].tolist(), ali, pc_col = 'Chunk')\n",
    "\n",
    "# create embeddings table for Lessons, drop redundant data in vectors column and convert to list for correct format\n",
    "vs_lessons = vector_store_faiss(lessons.drop(columns = ['vectors']), lessons['vectors'].tolist(), ali, pc_col = 'Lesson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Week': 12, 'Lesson': '8 summary for exam 2', 'Chunk': \"text retrieval and text mining. And text retrieval, as I explained,\\nis to help convert big text data into a small amount of most relevant data for\\na particular problem, and can also help providing knowledge provenance,\\nhelp interpreting patterns later. Text mining has to do with further\\nanalyzing the relevant data to discover the actionable knowledge that can be\\ndirectly useful for decision making or many other tasks. So this course covers text mining. And there's a companion course\\ncalled Text Retrieval and Search Engines that covers text retrieval. If you haven't taken that course,\\nit would be useful for you to take it, especially if you are interested\\nin building a text caching system. And taking both courses will give you\\na complete set of practical skills for building such a system. So in [INAUDIBLE]\\nI just would like to thank you for taking this course. I hope you have learned useful knowledge\\nand skills in test mining and [INAUDIBLE]. As you see from our discussions\"}, page_content=\"text retrieval and text mining. And text retrieval, as I explained,\\nis to help convert big text data into a small amount of most relevant data for\\na particular problem, and can also help providing knowledge provenance,\\nhelp interpreting patterns later. Text mining has to do with further\\nanalyzing the relevant data to discover the actionable knowledge that can be\\ndirectly useful for decision making or many other tasks. So this course covers text mining. And there's a companion course\\ncalled Text Retrieval and Search Engines that covers text retrieval. If you haven't taken that course,\\nit would be useful for you to take it, especially if you are interested\\nin building a text caching system. And taking both courses will give you\\na complete set of practical skills for building such a system. So in [INAUDIBLE]\\nI just would like to thank you for taking this course. I hope you have learned useful knowledge\\nand skills in test mining and [INAUDIBLE]. As you see from our discussions\"),\n",
       " Document(metadata={'Week': 1, 'Lesson': 'introduction video from previous semester', 'Chunk': \"the Text Retrieval and Text Mining. And these are the main techniques that we will cover in this course. Logically, in order to make use a lot of text data. We would first do text retrieval, and that's due to a large set of text data into a smaller but much more relevant set of data, that we actually need for a particular problem. And this step, is usually implemented by using text retrieval techniques that involve humans in the loop to find and locate the most relevant documents to a particular problem. Once we find the relevant documents, the next step is to do text mining, which is to further analyze the found of relevant documents to discover useful knowledge to extract the knowledge that can be directly used in application, especially in a application such as decision making. These two steps corresponding to Text Retrieval and Text Mining Techniques, that we will cover in this course. Based on this picture that I show you, this course is designed to leverage to corresponding\"}, page_content=\"the Text Retrieval and Text Mining. And these are the main techniques that we will cover in this course. Logically, in order to make use a lot of text data. We would first do text retrieval, and that's due to a large set of text data into a smaller but much more relevant set of data, that we actually need for a particular problem. And this step, is usually implemented by using text retrieval techniques that involve humans in the loop to find and locate the most relevant documents to a particular problem. Once we find the relevant documents, the next step is to do text mining, which is to further analyze the found of relevant documents to discover useful knowledge to extract the knowledge that can be directly used in application, especially in a application such as decision making. These two steps corresponding to Text Retrieval and Text Mining Techniques, that we will cover in this course. Based on this picture that I show you, this course is designed to leverage to corresponding\"),\n",
       " Document(metadata={'Week': 1, 'Lesson': 'introduction video from previous semester', 'Chunk': \"news articles, or Emails and other kind of documents and enterprise environment. Of course, we will also have a lot of scientific literature in text form. And nowadays, social media has been growing quickly. So we now see, tweets and other social media data also in the form of text. All such text data encode a lot of useful knowledge about the world because it's in some sense the data reported by human census about the observe the world. So we can analyzed this kind of data to discover a lot of useful knowledge. Especially the knowledge about the human opinions or preferences. So this kind of data is very useful and we can use computational methods to turn such data into useful knowledge, which can then be further used in many applications. The main techniques for making this happen include the Text Retrieval and Text Mining. And these are the main techniques that we will cover in this course. Logically, in order to make use a lot of text data. We would first do text retrieval, and\"}, page_content=\"news articles, or Emails and other kind of documents and enterprise environment. Of course, we will also have a lot of scientific literature in text form. And nowadays, social media has been growing quickly. So we now see, tweets and other social media data also in the form of text. All such text data encode a lot of useful knowledge about the world because it's in some sense the data reported by human census about the observe the world. So we can analyzed this kind of data to discover a lot of useful knowledge. Especially the knowledge about the human opinions or preferences. So this kind of data is very useful and we can use computational methods to turn such data into useful knowledge, which can then be further used in many applications. The main techniques for making this happen include the Text Retrieval and Text Mining. And these are the main techniques that we will cover in this course. Logically, in order to make use a lot of text data. We would first do text retrieval, and\"),\n",
       " Document(metadata={'Week': 12, 'Lesson': '8 summary for exam 2', 'Chunk': \"the original text to verify that. And that is why the search\\nengine is very important. Moreover, some techniques\\nof information retrieval, for example BM25, vector space and\\nare also very useful for text data mining. We only mention some of them,\\nbut if you know more about text retrieval you'll see that there\\nare many techniques that are used for it. Another technique that it's used for\\nis indexing technique that enables quick response of search engine to a user's\\nquery, and such techniques can be very useful for building efficient\\ntext mining systems as well. So, finally, I want to remind\\nyou of this big picture for harnessing big text data that I showed\\nyou at your beginning of the semester. So in general, to deal with\\na big text application system, we need two kinds text,\\ntext retrieval and text mining. And text retrieval, as I explained,\\nis to help convert big text data into a small amount of most relevant data for\"}, page_content=\"the original text to verify that. And that is why the search\\nengine is very important. Moreover, some techniques\\nof information retrieval, for example BM25, vector space and\\nare also very useful for text data mining. We only mention some of them,\\nbut if you know more about text retrieval you'll see that there\\nare many techniques that are used for it. Another technique that it's used for\\nis indexing technique that enables quick response of search engine to a user's\\nquery, and such techniques can be very useful for building efficient\\ntext mining systems as well. So, finally, I want to remind\\nyou of this big picture for harnessing big text data that I showed\\nyou at your beginning of the semester. So in general, to deal with\\na big text application system, we need two kinds text,\\ntext retrieval and text mining. And text retrieval, as I explained,\\nis to help convert big text data into a small amount of most relevant data for\")]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing content table\n",
    "vs_content.similarity_search('text retrieval and text mining')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Week': 1, 'Lesson': 'introduction video from previous semester'}, page_content='introduction video from previous semester'),\n",
       " Document(metadata={'Week': 5, 'Lesson': '5 6 link analysis part 1'}, page_content='5 6 link analysis part 1'),\n",
       " Document(metadata={'Week': 6, 'Lesson': '6 10 summary for exam 1'}, page_content='6 10 summary for exam 1'),\n",
       " Document(metadata={'Week': 9, 'Lesson': '9 latent dirichlet allocation lda part 1'}, page_content='9 latent dirichlet allocation lda part 1')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing lessons table\n",
    "vs_lessons.similarity_search('introduction video from previous semester')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vector_guardian",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
